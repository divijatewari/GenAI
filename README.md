sample.pdf
This project implements a Retrieval-Augmented Generation (RAG) pipeline that allows users to ask natural language questions about the contents of a PDF document. The system retrieves relevant information from the document and generates accurate answers using a local Large Language Model (LLM).

All sensitive information such as file names, API keys, internal traces, and reasoning steps are intentionally excluded to ensure privacy and safe sharing.

ğŸš€ Features

ğŸ“‘ Extracts text and tables from PDF documents

âœ‚ï¸ Splits content into overlapping chunks for better retrieval

ğŸ”— Generates semantic embeddings using Sentence Transformers

âš¡ Fast similarity search with FAISS

ğŸ¤– Local LLM-based answer generation

ğŸ” No exposure of private data, tool traces, or internal logs

ğŸ§  How It Works

PDF Extraction
Text and tables are extracted page by page from the input PDF.

Chunking
Extracted content is split into overlapping chunks to preserve context.

Embedding
Each chunk is converted into a vector representation using a sentence embedding model.

Indexing
FAISS is used to index embeddings for efficient similarity search.

Retrieval
For a user query, the most relevant chunks are retrieved from the index.

Answer Generation
A local LLM generates the final answer using only the retrieved context.

ğŸ“¦ Tech Stack

Python

pdfplumber â€“ PDF text & table extraction

SentenceTransformers â€“ Semantic embeddings

FAISS â€“ Vector similarity search

Local LLM API (e.g., Ollama)

ğŸ“ Project Structure
project/
â”‚
â”œâ”€â”€ main.py              # Core RAG pipeline
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ README.md            # Project documentation

â–¶ï¸ How to Run

Install dependencies

pip install -r requirements.txt


Start your local LLM service
(Ensure a compatible local LLM is running.)

Run the program

python main.py


Provide input

Enter the path to a PDF file



ğŸ¯ Use Cases

Academic PDF analysis

Policy or document Q&A

Knowledge extraction from reports

Offline or private document querying

Ask questions about its content

Type exit to quit

agenticrag.py
ğŸ¨ Agentic Multi-PDF Question Answering System (LangGraph)

This project implements an agent-based Retrieval-Augmented Generation (RAG) system that allows users to ask questions over multiple PDF documents using a structured LangGraph workflow.

The agent intelligently detects intent, retrieves relevant document content, generates grounded answers, and evaluates response quality â€” all in an interactive command-line interface.

âœ¨ Features

ğŸ“„ Loads multiple PDF documents

ğŸ“‘ Page-wise PDF text extraction

ğŸ§  Semantic search using OpenAI embeddings

ğŸ” Agentic workflow using LangGraph

ğŸ§ª Response evaluation with similarity scoring

ğŸ’¬ Interactive CLI chat experience

ğŸ” Secure API key handling via .env

ğŸ§  Agent Workflow

The system follows a state-driven agent loop:

User Input
   â†“
Intent Detection
   â†“
Document Retrieval
   â†“
Answer Generation
   â†“
Answer Evaluation
   â†“
Decision (Retrieve Again or End)


Each stage is implemented as a LangGraph node, making the system modular and extensible.

ğŸ—‚ï¸ Project Structure
project/
â”‚
â”œâ”€â”€ main.py              # Complete agent workflow implementation
â”œâ”€â”€ .env                 # Environment variables (API key)
â”œâ”€â”€ README.md            # Documentation
â”œâ”€â”€ *.pdf                # Input PDF documents

ğŸ“„ Document Handling

PDFs are parsed using Unstructured

Text is grouped page-wise

Each chunk stores:

PDF filename

Page number

Extracted text

This allows precise, source-aware answers.

ğŸ” Retrieval Strategy

User query is converted to an embedding

All document chunks are embedded

Cosine similarity ranks relevance

Top-K relevant chunks are selected

Keyword search is used as a fallback if embeddings fail

If no relevant data is found, the agent responds safely without hallucinating.

ğŸ§ª Answer Evaluation

After generating a response, the agent:

Embeds the answer

Compares it with retrieved context

Produces a confidence score (0.0 â€“ 1.0)

This score reflects how well the answer aligns with document content.

âš™ï¸ Tech Stack

Python

LangGraph â€“ agent orchestration

OpenAI API â€“ embeddings & generation

Unstructured â€“ PDF parsing

dotenv â€“ environment variable management

Typing / StateGraph â€“ structured agent state

â–¶ï¸ How to Run
1ï¸âƒ£ Install dependencies
pip install openai langgraph unstructured python-dotenv

2ï¸âƒ£ Set environment variables

Create a .env file:

OPENAI_API_KEY=your_api_key_here

3ï¸âƒ£ Add PDF files

Place your PDFs in the project directory (or update paths in the code).

4ï¸âƒ£ Run the application
python main.py

ğŸ’¬ Usage

Ask questions in natural language

The agent answers using only document knowledge

Confidence score is shown after each response

Type exit to quit

Example:

You: What are the refund rules for non-refundable tickets?
Agent: [Answer based on PDF content]
(Confidence: 0.81/1.00)

ğŸ” Security & Privacy

âœ… No API keys hard-coded

âœ… Uses environment variables

âœ… No external browsing by default

âœ… No internal reasoning exposed to users

Safe for:

College assignments

GitHub repositories

Project demos

ğŸš€ Possible Enhancements

Web UI (Streamlit / FastAPI)

Persistent vector storage

Multi-language document support

Better chunking strategies

PDF upload support

ğŸ“Œ Disclaimer

This system answers questions only based on the provided PDF documents.
If the information is not present, the agent clearly states that it does not know.




